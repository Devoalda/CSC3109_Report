@article{lapointe_primary_2018,
	title = {Primary brain tumours in adults},
	volume = {392},
	issn = {0140-6736},
	url = {https://www.sciencedirect.com/science/article/pii/S0140673618309905},
	doi = {10.1016/S0140-6736(18)30990-5},
	abstract = {Primary {CNS} tumours refer to a heterogeneous group of tumours arising from cells within the {CNS}, and can be benign or malignant. Malignant primary brain tumours remain among the most difficult cancers to treat, with a 5 year overall survival no greater than 35\%. The most common malignant primary brain tumours in adults are gliomas. Recent advances in molecular biology have improved understanding of glioma pathogenesis, and several clinically significant genetic alterations have been described. A number of these ({IDH}, 1p/19q codeletion, H3 Lys27Met, and {RELA}-fusion) are now combined with histology in the revised 2016 {WHO} classification of {CNS} tumours. It is likely that understanding such molecular alterations will contribute to the diagnosis, grading, and treatment of brain tumours. This progress in genomics, along with significant advances in cancer and {CNS} immunology, has defined a new era in neuro-oncology and holds promise for diagntic and therapeutic improvement. The challenge at present is to translate these advances into effective treatments. Current efforts are focused on developing molecular targeted therapies, immunotherapies, gene therapies, and novel drug-delivery technologies. Results with single-agent therapies have been disappointing so far, and combination therapies seem to be required to achieve a broad and durable antitumour response. Biomarker-targeted clinical trials could improve efficiencies of therapeutic development.},
	pages = {432--446},
	number = {10145},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Lapointe, Sarah and Perry, Arie and Butowski, Nicholas A},
	urldate = {2024-06-02},
	date = {2018-08-04},
}

@article{nalepa_data_2019,
	title = {Data Augmentation for Brain-Tumor Segmentation: A Review},
	volume = {13},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2019.00083},
	doi = {10.3389/fncom.2019.00083},
	shorttitle = {Data Augmentation for Brain-Tumor Segmentation},
	abstract = {Data augmentation is a popular technique which helps improve generalization capabilities of deep neural networks, and can be perceived as implicit regularization. It plays a pivotal role in scenarios in which the amount of high-quality ground-truth data is limited, and acquiring new examples is costly and time-consuming. This is a very common problem in medical image analysis, especially tumor delineation. In this paper, we review the current advances in data-augmentation techniques applied to magnetic resonance images of brain tumors. To better understand the practical aspects of such algorithms, we investigate the papers submitted to the Multimodal Brain Tumor Segmentation Challenge ({BraTS} 2018 edition), as the {BraTS} dataset became a standard benchmark for validating existent and emerging brain-tumor detection and segmentation techniques. We verify which data augmentation approaches were exploited and what was their impact on the abilities of underlying supervised learners. Finally, we highlight the most promising research directions to follow in order to synthesize high-quality artificial brain-tumor examples which can boost the generalization abilities of deep models.},
	journaltitle = {Frontiers in Computational Neuroscience},
	shortjournal = {Front. Comput. Neurosci.},
	author = {Nalepa, Jakub and Marcinkiewicz, Michal and Kawulok, Michal},
	urldate = {2024-06-02},
	date = {2019-12-11},
	keywords = {Data augmentation, deep learning, Deep neural network, image segmentation, {MRI}},
}

@article{doi:10.1148/ryai.230095,
author = {Mahmutoglu, Mustafa Ahmed and Preetha, Chandrakanth Jayachandran and Meredig, Hagen and Tonn, Joerg-Christian and Weller, Michael and Wick, Wolfgang and Bendszus, Martin and Brugnara, Gianluca and Vollmuth, Philipp},
title = {Deep Learning–based Identification of Brain MRI Sequences Using a Model Trained on Large Multicentric Study Cohorts},
journal = {Radiology: Artificial Intelligence},
volume = {6},
number = {1},
pages = {e230095},
year = {2024},
doi = {10.1148/ryai.230095},
    note ={PMID: 38166331},

URL = { 
    
        https://doi.org/10.1148/ryai.230095
    
    

},
eprint = { 
    
        https://doi.org/10.1148/ryai.230095
    
    

}
,
    abstract = { Purpose To develop a fully automated device- and sequence-independent convolutional neural network (CNN) for reliable and high-throughput labeling of heterogeneous, unstructured MRI data. Materials and Methods Retrospective, multicentric brain MRI data (2179 patients with glioblastoma, 8544 examinations, 63 327 sequences) from 249 hospitals and 29 scanner types were used to develop a network based on ResNet-18 architecture to differentiate nine MRI sequence types, including T1-weighted, postcontrast T1-weighted, T2-weighted, fluid-attenuated inversion recovery, susceptibility-weighted, apparent diffusion coefficient, diffusion-weighted (low and high b value), and gradient-recalled echo T2*-weighted and dynamic susceptibility contrast-related images. The two-dimensional-midsection images from each sequence were allocated to training or validation (approximately 80\%) and testing (approximately 20\%) using a stratified split to ensure balanced groups across institutions, patients, and MRI sequence types. The prediction accuracy was quantified for each sequence type, and subgroup comparison of model performance was performed using χ2 tests. Results On the test set, the overall accuracy of the CNN (ResNet-18) ensemble model among all sequence types was 97.9\% (95\% CI: 97.6, 98.1), ranging from 84.2\% for susceptibility-weighted images (95\% CI: 81.8, 86.6) to 99.8\% for T2-weighted images (95\% CI: 99.7, 99.9). The ResNet-18 model achieved significantly better accuracy compared with ResNet-50 despite its simpler architecture (97.9\% vs 97.1\%; P ≤ .001). The accuracy of the ResNet-18 model was not affected by the presence versus absence of tumor on the two-dimensional-midsection images for any sequence type (P > .05). Conclusion The developed CNN (www.github.com/neuroAI-HD/HD-SEQ-ID) reliably differentiates nine types of MRI sequences within multicenter and large-scale population neuroimaging data and may enhance the speed, accuracy, and efficiency of clinical and research neuroradiologic workflows. Keywords: MR-Imaging, Neural Networks, CNS, Brain/Brain Stem, Computer Applications-General (Informatics), Convolutional Neural Network (CNN), Deep Learning Algorithms, Machine Learning Algorithms Supplemental material is available for this article. © RSNA, 2023 }
}

@article{Islam_Barua_Rahman_Ahammed_Akter_Uddin_2023, title={Transfer learning architectures with fine-tuning for brain tumor classification using Magnetic Resonance Imaging}, volume={4}, DOI={10.1016/j.health.2023.100270}, journal={Healthcare Analytics}, author={Islam, Md. Monirul and Barua, Prema and Rahman, Moshiur and Ahammed, Tanvir and Akter, Laboni and Uddin, Jia}, year={2023}, month={Dec}, pages={100270}} 

@InProceedings{10.1007/978-3-030-32695-1_7,
author="Thomas, Armin W.
and M{\"u}ller, Klaus-Robert
and Samek, Wojciech",
editor="Zhou, Luping
and Sarikaya, Duygu
and Kia, Seyed Mostafa
and Speidel, Stefanie
and Malpani, Anand
and Hashimoto, Daniel
and Habes, Mohamad
and L{\"o}fstedt, Tommy
and Ritter, Kerstin
and Wang, Hongzhi",
title="Deep Transfer Learning for Whole-Brain FMRI Analyses",
booktitle="OR 2.0 Context-Aware Operating Theaters and Machine Learning in Clinical Neuroimaging",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="59--67",
abstract="The application of deep learning (DL) models to the decoding of cognitive states from whole-brain functional Magnetic Resonance Imaging (fMRI) data is often hindered by the small sample size and high dimensionality of these datasets. Especially, in clinical settings, where patient data are scarce. In this work, we demonstrate that transfer learning represents a solution to this problem. Particularly, we show that a DL model, which has been previously trained on a large openly available fMRI dataset of the Human Connectome Project, outperforms a model variant with the same architecture, but which is trained from scratch, when both are applied to the data of a new, unrelated fMRI task. The pre-trained DL model variant is able to correctly decode 67.51{\%} of the cognitive states from a test dataset with 100 individuals, when fine-tuned on a dataset of the size of only three subjects.",
isbn="978-3-030-32695-1"
}

@INPROCEEDINGS{10125766,
  author={Mitta, Anirudh B. and Hegde, Ajay H. and P., Asha Rani K. and S., Gowrishankar},
  booktitle={2023 7th International Conference on Trends in Electronics and Informatics (ICOEI)}, 
  title={Brain Tumor Detection: An Application based on Transfer Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1424-1430},
  keywords={Deep learning;Training;Magnetic resonance imaging;Transfer learning;Brain modeling;Data models;Classification algorithms;Brain Tumor;Machine Learning;Transfer Learning;Models;Accuracy;Precision;Convolution Neural Network;Magnetic Resonance Imaging},
  doi={10.1109/ICOEI56765.2023.10125766}}


@article{Paul2017Deep,title={Deep learning for brain tumor classification},author={Justin S. Paul and A. Plassard and B. Landman and D. Fabbri},year={2017},volume={10137},doi={10.1117/12.2254195}}

@article{Fahimi2020Generative,title={Generative Adversarial Networks-Based Data Augmentation for Brain–Computer Interface},author={Fatemeh Fahimi and S. Došen and K. Ang and N. Mrachacz‐Kersting and Cuntai Guan},journal={IEEE Transactions on Neural Networks and Learning Systems},year={2020},volume={32},pages={4039-4051},doi={10.1109/tnnls.2020.3016666}}

@INPROCEEDINGS{10183465,
  author={Krishna, Divi Leela and Dedeepya Padmanabhuni, Naga Venkata and JayaLakshmi, G},
  booktitle={2023 International Conference on Computational Intelligence and Sustainable Engineering Solutions (CISES)}, 
  title={Data Augmentation Based Brain Tumor Detection Using CNN and Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={317-321},
  keywords={Deep learning;Machine learning algorithms;Magnetic resonance imaging;Logic gates;Brain modeling;Data augmentation;Data models;Brain Tumor;ResNet-50;VGG-16;DenseNet-121;Magnetic Resonance Imaging;Convolutional Neural Networks;Data Augmentation},
  doi={10.1109/CISES58720.2023.10183465}}
  
  @article{Asif2022Improving,title={Improving Effectiveness of Different Deep Transfer Learning-Based Models for Detecting Brain Tumors From MR Images},author={S. Asif and Wenhui Yi and Q. Ain and Jinhai Hou and Tao Yi and Jinhai Si},journal={IEEE Access},year={2022},volume={10},pages={34716-34730},doi={10.1109/ACCESS.2022.3153306}}


@article{lenchik_automated_2019,
	title = {Automated Segmentation of Tissues Using {CT} and {MRI}: A Systematic Review},
	volume = {26},
	issn = {1076-6332, 1878-4046},
	url = {https://www.academicradiology.org/article/S1076-6332(19)30353-8/abstract},
	doi = {10.1016/j.acra.2019.07.006},
	shorttitle = {Automated Segmentation of Tissues Using {CT} and {MRI}},
	pages = {1695--1706},
	number = {12},
	journaltitle = {Academic Radiology},
	shortjournal = {Academic Radiology},
	author = {Lenchik, Leon and Heacock, Laura and Weaver, Ashley A. and Boutin, Robert D. and Cook, Tessa S. and Itri, Jason and Filippi, Christopher G. and Gullapalli, Rao P. and Lee, James and Zagurovskaya, Marianna and Retson, Tara and Godwin, Kendra and Nicholson, Joey and Narayana, Ponnada A.},
	urldate = {2024-06-08},
	date = {2019-12-01},
	pmid = {31405724},
	note = {Publisher: Elsevier},
	keywords = {background parenchymal enhancement, {BPE}, {CNN}, convolutional neural network, {CT}, Dice Similarity Coefficient, {DSC}, {FGT}, fibroglandular tissue, machine learning, Machine learning, Medical Image Computing and Computer Assisted Intervention, {MICCAI}, {ML}, {MRI}, Quantitative imaging, Segmentation},
}

  
@article{iorgulescu_misclassification_2019,
	title = {The Misclassification of Diffuse Gliomas: Rates and Outcomes},
	volume = {25},
	issn = {1078-0432},
	url = {https://doi.org/10.1158/1078-0432.CCR-18-3101},
	doi = {10.1158/1078-0432.CCR-18-3101},
	shorttitle = {The Misclassification of Diffuse Gliomas},
	abstract = {The integrated histopathologic and molecular diagnoses of the 2016 {WHO} classification of central nervous system tumors have revolutionized patient care by improving diagnostic accuracy and reproducibility; however, the frequency and consequences of misclassification of histologically diagnosed diffuse gliomas are unknown.Patients with newly diagnosed {ICD}-O-3 (International Classification of Diseases) histologically encoded diffuse gliomas from 2010–2015 were identified from the National Cancer Database, the misclassification rates and overall survival ({OS}) of which were assessed by {WHO} grade and 1p/19q status. In addition, misclassification rates by isocitrate dehydrogenase ({IDH}), {ATRX}, and p53 statuses were examined in an analogous multi-institutional cohort of registry-encoded diffuse gliomas.Of 74,718 patients with diffuse glioma, only 74.4\% and 78.8\% of molecularly characterized {WHO} grade {II} and {III} oligodendrogliomas were in fact 1p/19q-codeleted. In addition, 28.9\% and 36.8\% of histologically encoded grade {II} and {III} “oligoastrocytomas”, and 6.3\% and 8.8\% of grade {II} and {III} astrocytomas had 1p/19q-codeletion, thus molecularly representing oligodendrogliomas if also {IDH} mutant. {OS} significantly depended on accurate {WHO} grading and 1p/19q status.On the basis of 1p/19q, {IDH}, {ATRX}, and p53, the misclassification rates of histologically encoded oligodendrogliomas, astrocytomas, and glioblastomas are approximately 21\%–35\%, 6\%–9\%, and 9\%, respectively; with significant clinical implications. Our findings suggest that when compared with historical histology-only classified data, in national registry, as well as, institutional databases, there is the potential for false-positive results in contemporary trials of molecularly classified diffuse gliomas, which could contribute to a seemingly positive phase {II} trial (based on historical comparison) failing at the phase {III} stage. Critically, findings from diffuse glioma clinical trials and historical cohorts using prior histology-only {WHO} schemes must be cautiously reinterpreted.},
	pages = {2656--2663},
	number = {8},
	journaltitle = {Clinical Cancer Research},
	shortjournal = {Clinical Cancer Research},
	author = {Iorgulescu, J. Bryan and Torre, Matthew and Harary, Maya and Smith, Timothy R. and Aizer, Ayal A. and Reardon, David A. and Barnholtz-Sloan, Jill S. and Perry, Arie},
	urldate = {2024-06-08},
	date = {2019-04-15},
}

  
@article{rolnick_tackling_2022,
	title = {Tackling Climate Change with Machine Learning},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3485128},
	doi = {10.1145/3485128},
	abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning ({ML}) experts, may wonder how we can help. Here we describe how {ML} can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by {ML}, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the {ML} community to join the global effort against climate change.},
	pages = {42:1--42:96},
	number = {2},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra Sasha and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
	urldate = {2024-06-08},
	date = {2022-02-07},
	keywords = {adaptation, artificial intelligence, Climate change, machine learning, mitigation},
}

  
@article{doupe_machine_2019,
	title = {Machine Learning for Health Services Researchers},
	volume = {22},
	issn = {1098-3015, 1524-4733},
	url = {https://www.valueinhealthjournal.com/article/S1098-3015(19)30146-9/fulltext?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1098301519301469%3Fshowall%3Dtrue},
	doi = {10.1016/j.jval.2019.02.012},
	pages = {808--815},
	number = {7},
	journaltitle = {Value in Health},
	shortjournal = {Value in Health},
	author = {Doupe, Patrick and Faghmous, James and Basu, Sanjay},
	urldate = {2024-06-08},
	date = {2019-07-01},
	pmid = {31277828},
	note = {Publisher: Elsevier},
	keywords = {claims data, deep learning, elastic net, gradient boosting machine, gradient forest, health services research, machine learning, neural networks, random forest},
}

  
@article{nazar_systematic_2021,
	title = {A Systematic Review of Human–Computer Interaction and Explainable Artificial Intelligence in Healthcare With Artificial Intelligence Techniques},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9614151},
	doi = {10.1109/ACCESS.2021.3127881},
	abstract = {Artificial intelligence ({AI}) is one of the emerging technologies. In recent decades, artificial intelligence ({AI}) has gained widespread acceptance in a variety of fields, including virtual support, healthcare, and security. Human-Computer Interaction ({HCI}) is a field that has been combining {AI} and human-computer engagement over the past several years in order to create an interactive intelligent system for user interaction. {AI}, in conjunction with {HCI}, is being used in a variety of fields by employing various algorithms and employing {HCI} to provide transparency to the user, allowing them to trust the machine. The comprehensive examination of both the areas of {AI} and {HCI}, as well as their subfields, has been explored in this work. The main goal of this article was to discover a point of intersection between the two fields. The understanding of Explainable Artificial Intelligence ({XAI}), which is a linking point of {HCI} and {XAI}, was gained through a literature review conducted in this research. The literature survey encompassed themes identified in the literature (such as {XAI} and its areas, major {XAI} aims, and {XAI} problems and challenges). The study’s other major focus was on the use of {AI}, {HCI}, and {XAI} in healthcare. The poll also addressed the shortcomings in {XAI} in healthcare, as well as the field’s future potential. As a result, the literature indicates that {XAI} in healthcare is still a novel subject that has to be explored more in the future.},
	pages = {153316--153348},
	journaltitle = {{IEEE} Access},
	author = {Nazar, Mobeen and Alam, Muhammad Mansoor and Yafi, Eiad and Su’ud, Mazliham Mohd},
	urldate = {2024-06-08},
	date = {2021},
	note = {Conference Name: {IEEE} Access},
	keywords = {Artificial intelligence, Data models, deep learning, explainable artificial intelligence, healthcare, Human computer interaction, human-centered design, human-computer interaction, machine learning, Medical services, Security, Systematics, usability, Usability, user-centered design},
}


  
@inproceedings{imtiaz_brain_2023,
	title = {Brain Tumor Segmentation from {MR} Images using Customized U-net for a Smaller Dataset},
	url = {https://ieeexplore.ieee.org/document/10389092},
	doi = {10.1109/BioCAS58349.2023.10389092},
	abstract = {In medical image analysis, deep learning has emerged as a powerful tool for solving complex tasks such as segmentation. This research presents an original approach using a customized U-net model for automatic brain tumor segmentation in Magnetic resonance imaging scans. The model's performance is thoroughly assessed using metrics like Mean Dice Similarity Coefficient, Sensitivity, Specificity, and Accuracy. The study evaluates the model's proficiency on 50 test images from in-house and established {BRaTS} 2020 datasets. The proposed U-net model showcases its superiority by achieving high Dice Similarity Coefficient scores, indicating a solid alignment with reference segmentations. It also demonstrates impressive sensitivity and specificity values, signifying its capacity to capture tumor regions and true negatives accurately. The results demonstrate that the proposed model effectively segments tumors on a relatively smaller dataset.},
	eventtitle = {2023 {IEEE} Biomedical Circuits and Systems Conference ({BioCAS})},
	pages = {1--5},
	booktitle = {2023 {IEEE} Biomedical Circuits and Systems Conference ({BioCAS})},
	author = {Imtiaz, Romil and Mirza, Muhammad Waqar and Siddiq, Asif and Farooq-i-Azam, Muhammad and Khan, Ishtiaq Rasool and Rahardja, Susanto},
	urldate = {2024-05-28},
	date = {2023-10},
	note = {{ISSN}: 2766-4465},
	keywords = {Adaptation models, Brain modeling, Brain tumor segmentation, Differential evolution, Image segmentation, Morphological operations, Sensitivity, Sensitivity and specificity, Solid modeling, Solids, Tumor detection},
}

@article{abd-ellah_automatic_2024,
	title = {Automatic brain-tumor diagnosis using cascaded deep convolutional neural networks with symmetric U-Net and asymmetric residual-blocks},
	volume = {14},
	rights = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-59566-7},
	doi = {10.1038/s41598-024-59566-7},
	abstract = {The use of various kinds of magnetic resonance imaging ({MRI}) techniques for examining brain tissue has increased significantly in recent years, and manual investigation of each of the resulting images can be a time-consuming task. This paper presents an automatic brain-tumor diagnosis system that uses a {CNN} for detection, classification, and segmentation of glioblastomas; the latter stage seeks to segment tumors inside glioma {MRI} images. The structure of the developed multi-unit system consists of two stages. The first stage is responsible for tumor detection and classification by categorizing brain {MRI} images into normal, high-grade glioma (glioblastoma), and low-grade glioma. The uniqueness of the proposed network lies in its use of different levels of features, including local and global paths. The second stage is responsible for tumor segmentation, and skip connections and residual units are used during this step. Using 1800 images extracted from the {BraTS} 2017 dataset, the detection and classification stage was found to achieve a maximum accuracy of 99\%. The segmentation stage was then evaluated using the Dice score, specificity, and sensitivity. The results showed that the suggested deep-learning-based system ranks highest among a variety of different strategies reported in the literature.},
	pages = {9501},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Abd-Ellah, Mahmoud Khaled and Awad, Ali Ismail and Khalaf, Ashraf A. M. and Ibraheem, Amira Mofreh},
	urldate = {2024-06-02},
	date = {2024-04-25},
	langid = {english},
	keywords = {Computer science, Biomedical engineering, Design, Magnetic resonance imaging, Software, synthesis and processing},
}

@inproceedings{ding_slf-unet_2024,
	location = {Cham},
	title = {{SLf}-{UNet}: Improved {UNet} for Brain {MRI} Segmentation by Combining Spatial and Low-Frequency Domain Features},
	isbn = {978-3-031-50075-6},
	doi = {10.1007/978-3-031-50075-6_32},
	shorttitle = {{SLf}-{UNet}},
	abstract = {Deep learning-based methods have shown remarkable performance in brain tumor image segmentation. However, there is a lack of research on segmenting brain tumor lesions using frequency domain features of images. To address this gap, an improved network {SLf}-{UNet} has been proposed in this paper, which is a two-dimensional encoder-decoder architecture combining spatial and low-frequency domain features based on U-Net. The proposed model effectively learns information from spatial and frequency domains. Herein, we present a novel upsample approach by using zero padding in the high-frequency region and replacing the part of the convolution operation with a convolution block combining spatial frequency domain features. Our experimental results demonstrate that our method outperforms current mainstream approaches on {BraTS} 2019 and {BraTS} 2020 datasets. Code is available soon at https://github.com/{noseDewdrop}/{SLf}-{UNet}.},
	pages = {415--426},
	booktitle = {Advances in Computer Graphics},
	publisher = {Springer Nature Switzerland},
	author = {Ding, Hui and Lu, Jiacheng and Cai, Junwei and Zhang, Yawei and Shang, Yuanyuan},
	editor = {Sheng, Bin and Bi, Lei and Kim, Jinman and Magnenat-Thalmann, Nadia and Thalmann, Daniel},
	date = {2024},
	langid = {english},
}

  
@misc{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	number = {{arXiv}:1505.04597},
	publisher = {{arXiv}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2024-06-08},
	date = {2015-05-18},
	eprinttype = {arxiv},
	eprint = {1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_unet_2018,
	title = {{UNet}++: A Nested U-Net Architecture for Medical Image Segmentation},
	url = {http://arxiv.org/abs/1807.10165},
	doi = {10.48550/arXiv.1807.10165},
	shorttitle = {{UNet}++},
	abstract = {In this paper, we present {UNet}++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated {UNet}++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose {CT} scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal {CT} scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that {UNet}++ with deep supervision achieves an average {IoU} gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
	number = {{arXiv}:1807.10165},
	publisher = {{arXiv}},
	author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
	urldate = {2024-06-08},
	date = {2018-07-18},
	eprinttype = {arxiv},
	eprint = {1807.10165 [cs, eess, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}
  
@article{hastomo_classification_2024,
	title = {Classification of Brain Image Tumor using {EfficientNet} B1-B2 Deep Learning},
	volume = {27},
	doi = {10.18196/st.v27i1.19691},
	abstract = {In this study, a new neural network model ({EfficientNet} B1-B2) was sought for the detection of brain tumors in magnetic resonance imaging ({MRI}) images. The primary objective was to achieve high accuracy rates so as to classify the images. The deep learning techniques meticulously processed and increased the data augmentation as much as possible for the {EfficientNet} B1-B2 models. Our experimental results show an accuracy of 98\% in the B1 version in Table {II}. This provides a potentially optimistic view of the application of artificial intelligence technology to disease diagnosis based on medical image analysis. Nonetheless, we must remind ourselves that the dataset we used has limitations in terms of the challenges it can pose. Although the number of potential variations of actual medical images constitutes a major challenge, it is not the only one. Most medical datasets are unbalanced, contain highly variable noise, have a slow internal structure, and are often small in size. Hence, our end goal is to help stimulate not only the field of brain tumor detection and treatment but also the development of more sophisticated classification models in the health context.},
	pages = {46--54},
	journaltitle = {Semesta Teknika},
	shortjournal = {Semesta Teknika},
	author = {Hastomo, Widi and Satyo, Adhitio and Sestri, Ellya and Terisia, Vany and Yusuf, Diana and Arman, Shevty and Arif, Dodi},
	date = {2024-05-02},
}


@article{zhou2019unetplusplus,
  title={UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation},
  author={Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  journal={IEEE Transactions on Medical Imaging},
  year={2019},
  publisher={IEEE}
}

@incollection{zhou2018unetplusplus,
  title={Unet++: A Nested U-Net Architecture for Medical Image Segmentation},
  author={Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  booktitle={Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support},
  pages={3--11},
  year={2018},
  publisher={Springer}
}

@phdthesis{zhou2021towards,
  title={Towards Annotation-Efficient Deep Learning for Computer-Aided Diagnosis},
  author={Zhou, Zongwei},
  year={2021},
  school={Arizona State University}
}

@misc{Yakubovskiy_2019,
  author = {Pavel Iakubovskii},
  title = {Segmentation Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/qubvel/segmentation_models}}
}

@misc{liu_variance_2019,
	title = {On the Variance of the Adaptive Learning Rate and Beyond},
	url = {http://arxiv.org/abs/1908.03265},
	doi = {10.48550/arXiv.1908.03265},
	abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like {RMSprop} and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose {RAdam}, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/{LiyuanLucasLiu}/{RAdam}.},
	number = {{arXiv}:1908.03265},
	publisher = {{arXiv}},
	author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
	urldate = {2024-06-06},
	date = {2019-08-08},
	eprinttype = {arxiv},
	eprint = {1908.03265 [cs, stat]},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
}

@inproceedings{baheti_eff-unet_2020,
	location = {Seattle, {WA}, {USA}},
	title = {Eff-{UNet}: A Novel Architecture for Semantic Segmentation in Unstructured Environment},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72819-360-1},
	url = {https://ieeexplore.ieee.org/document/9150621/},
	doi = {10.1109/CVPRW50498.2020.00187},
	shorttitle = {Eff-{UNet}},
	abstract = {Since the last few decades, the number of road causalities has seen continuous growth across the globe. Nowadays intelligent transportation systems are being developed to enable safe and relaxed driving and scene understanding of the surrounding environment is an integral part of it. While several approaches are being developed for semantic scene segmentation based on deep learning and Convolutional Neural Network ({CNN}), these approaches assume well structured road infrastructure and driving environment. We focus our work on recent India Driving Lite Dataset ({IDD}), which contains data from unstructured driving environment and was hosted as an online challenge in {NCVPRIPG} 2019. We propose a novel architecture named as Eff-{UNet} which combines the effectiveness of compound scaled {EfﬁcientNet} as the encoder for feature extraction with {UNet} decoder for reconstructing the ﬁne-grained segmentation map. High level feature information as well as low level spatial information useful for precise segmentation are combined. The proposed architecture achieved 0.7376 and 0.6276 mean Intersection over Union ({mIoU}) on validation and test dataset respectively and won ﬁrst prize in {IDD} lite segmentation challenge outperforming other approaches in the literature.},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	pages = {1473--1481},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	publisher = {{IEEE}},
	author = {Baheti, Bhakti and Innani, Shubham and Gajre, Suhas and Talbar, Sanjay},
	urldate = {2024-06-16},
	date = {2020-06},
	langid = {english},
}

@inproceedings{yu_pneumonia_2021,
	title = {Pneumonia Detection with U-{EfficientNet}},
	url = {https://ieeexplore.ieee.org/document/9750509},
	doi = {10.1109/DSC53577.2021.00093},
	abstract = {During the {COVID}-19, with a surge of the number of patients, it is of great importance to identify medical images quickly and accurately and find out suspected new patients to improve the diagnosis efficiency In this study, based on the review of the commonly used model including {ResNet}, {YOLO}, {UNet}, we proposed U-{EfficientNet} model to fulfill this task. We have also used a novel metric to help us better measure the ability of our model, which combined {IoU}, precision and recall. The result shows our model has made improvement in both efficiency and accuracy.},
	eventtitle = {2021 {IEEE} Sixth International Conference on Data Science in Cyberspace ({DSC})},
	pages = {591--594},
	booktitle = {2021 {IEEE} Sixth International Conference on Data Science in Cyberspace ({DSC})},
	author = {Yu, Zechuan},
	urldate = {2024-06-16},
	date = {2021-10},
	keywords = {{COVID}-19, Data models, Data science, Deep learning, Deep Learning, Feature extraction, Measurement, Pneumonia Detection, Pulmonary diseases, {UNet}},
}

@article{lin_brain_2024,
	title = {Brain tumor segmentation using U-Net in conjunction with {EfficientNet}},
	volume = {10},
	issn = {2376-5992},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773611/},
	doi = {10.7717/peerj-cs.1754},
	abstract = {According to the Ten Leading Causes of Death Statistics Report by the Ministry of Health and Welfare in 2021, cancer ranks as the leading cause of mortality. Among them, pleomorphic glioblastoma is a common type of brain cancer. Brain cancer often occurs in the brain with unclear boundaries from normal brain tissue, necessitating assistance from experienced doctors to distinguish brain tumors before surgical resection to avoid damaging critical neural structures. In recent years, with the advancement of deep learning ({DL}) technology, artificial intelligence ({AI}) plays a vital role in disease diagnosis, especially in the field of image segmentation. This technology can aid doctors in locating and measuring brain tumors, while significantly reducing manpower and time costs. Currently, U-Net is one of the primary image segmentation techniques. It utilizes skip connections to combine high-level and low-level feature information, leading to significant improvements in segmentation accuracy. To further enhance the model’s performance, this study explores the feasibility of using {EfficientNetV}2 as an encoder in combination with U-net. Experimental results indicate that employing {EfficientNetV}2 as an encoder together with U-net can improve the segmentation model’s Dice score (loss = 0.0866, accuracy = 0.9977, and Dice similarity coefficient ({DSC}) = 0.9133).},
	pages = {e1754},
	journaltitle = {{PeerJ} Computer Science},
	shortjournal = {{PeerJ} Comput Sci},
	author = {Lin, Shu-You and Lin, Chun-Ling},
	urldate = {2024-06-16},
	date = {2024-01-02},
	pmid = {38196955},
	pmcid = {PMC10773611},
}

@article{khaliki_brain_2024,
	title = {Brain tumor detection from images and comparison with transfer learning methods and 3-layer {CNN}},
	volume = {14},
	rights = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-52823-9},
	doi = {10.1038/s41598-024-52823-9},
	abstract = {Health is very important for human life. In particular, the health of the brain, which is the executive of the vital resource, is very important. Diagnosis for human health is provided by magnetic resonance imaging ({MRI}) devices, which help health decision makers in critical organs such as brain health. Images from these devices are a source of big data for artificial intelligence. This big data enables high performance in image processing classification problems, which is a subfield of artificial intelligence. In this study, we aim to classify brain tumors such as glioma, meningioma, and pituitary tumor from brain {MR} images. Convolutional Neural Network ({CNN}) and {CNN}-based inception-V3, {EfficientNetB}4, {VGG}19, transfer learning methods were used for classification. F-score, recall, imprinting and accuracy were used to evaluate these models. The best accuracy result was obtained with {VGG}16 with 98\%, while the F-score value of the same transfer learning model was 97\%, the Area Under the Curve ({AUC}) value was 99\%, the recall value was 98\%, and the precision value was 98\%. {CNN} architecture and {CNN}-based transfer learning models are very important for human health in early diagnosis and rapid treatment of such diseases.},
	pages = {2664},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Khaliki, Mohammad Zafer and Başarslan, Muhammet Sinan},
	urldate = {2024-06-05},
	date = {2024-02-01},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Cancer},
}

@misc{szegedy_rethinking_2015,
	title = {Rethinking the Inception Architecture for Computer Vision},
	url = {http://arxiv.org/abs/1512.00567},
	doi = {10.48550/arXiv.1512.00567},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the {ILSVRC} 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
	number = {{arXiv}:1512.00567},
	publisher = {{arXiv}},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	urldate = {2024-06-06},
	date = {2015-12-11},
	eprinttype = {arxiv},
	eprint = {1512.00567 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}


@article{Tummala2022,
  author    = {Tummala, S. and Kadry, S. and Bukhari, S. A. C. and Rauf, H. T.},
  title     = {Classification of Brain Tumor from Magnetic Resonance Imaging Using Vision Transformers Ensembling},
  journal   = {Current Oncology},
  year      = {2022},
  month     = {Oct},
  day       = {7},
  volume    = {Toronto, Ont.},
  url       = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9600395/}
}


@article{Asiri2023Advancing,
title={Advancing Brain Tumor Classification through Fine-Tuned Vision Transformers: A Comparative Study of Pre-Trained Models},
author={Abdullah A. Asiri and Ahmad Shaf and Tariq Ali and Muhammad Ahmad Pasha and M. Aamir and Muhammad Irfan and Saeed Alqahtani and A. Alghamdi and Ali H. Alghamdi and A. F. Alshamrani and Magbool Alelyani and Sultan Alamri},
journal={Sensors (Basel, Switzerland)},
year={2023},
volume={23},
doi={10.3390/s23187913}}


@article{Diker2021A,
title={A Performance Comparison of Pre-trained Deep Learning Models to Classify Brain Tumor},
author={A. Diker},
journal={IEEE EUROCON 2021 - 19th International Conference on Smart Technologies},
year={2021},
pages={246-249},
doi={10.1109/EUROCON52738.2021.9535636}}


@article{Khan2021Transformers,
title={Transformers in Vision: A Survey},
author={Salman Hameed Khan and Muzammal Naseer and Munawar Hayat and Syed Waqas Zamir and F. Khan and M. Shah},
journal={ACM Computing Surveys (CSUR)},
year={2021},
volume={54},
pages={1 - 41},
doi={10.1145/3505244}}

@article{He_Zhang_Ren_Sun_2015, title={Deep residual learning for image recognition}, url={https://arxiv.org/abs/1512.03385}, DOI={10.48550/arxiv.1512.03385}, journal={arXiv (Cornell University)}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, year={2015}, month=jan }


@article{Wu2020Visual,
title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},
author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Masayoshi Tomizuka and Kurt Keutzer and Peter Vajda},
journal={ArXiv},
year={2020},
doi={}}

@article{Matsoukas2021Is,
title={Is it Time to Replace CNNs with Transformers for Medical Images?},
author={Christos Matsoukas and Johan Fredin Haslum and Magnus P Soderberg and Kevin Smith},
journal={ArXiv},
year={2021},
volume={abs/2108.09038},
doi={}}

@article{Simon2022Vision,
title={Vision Transformers for Brain Tumor Classification},
author={Eliott Simon and A. Briassouli},
year={2022},
pages={123-130},
doi={10.5220/0010834300003123}}


@Article{brainsci13091320,
AUTHOR = {Rasheed, Zahid and Ma, Yong-Kui and Ullah, Inam and Ghadi, Yazeed Yasin and Khan, Muhammad Zubair and Khan, Muhammad Abbas and Abdusalomov, Akmalbek and Alqahtani, Fayez and Shehata, Ahmed M.},
TITLE = {Brain Tumor Classification from MRI Using Image Enhancement and Convolutional Neural Network Techniques},
JOURNAL = {Brain Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {9},
ARTICLE-NUMBER = {1320},
URL = {https://www.mdpi.com/2076-3425/13/9/1320},
PubMedID = {37759920},
ISSN = {2076-3425},
ABSTRACT = {The independent detection and classification of brain malignancies using magnetic resonance imaging (MRI) can present challenges and the potential for error due to the intricate nature and time-consuming process involved. The complexity of the brain tumor identification process primarily stems from the need for a comprehensive evaluation spanning multiple modules. The advancement of deep learning (DL) has facilitated the emergence of automated medical image processing and diagnostics solutions, thereby offering a potential resolution to this issue. Convolutional neural networks (CNNs) represent a prominent methodology in visual learning and image categorization. The present study introduces a novel methodology integrating image enhancement techniques, specifically, Gaussian-blur-based sharpening and Adaptive Histogram Equalization using CLAHE, with the proposed model. This approach aims to effectively classify different categories of brain tumors, including glioma, meningioma, and pituitary tumor, as well as cases without tumors. The algorithm underwent comprehensive testing using benchmarked data from the published literature, and the results were compared with pre-trained models, including VGG16, ResNet50, VGG19, InceptionV3, and MobileNetV2. The experimental findings of the proposed method demonstrated a noteworthy classification accuracy of 97.84\%, a precision success rate of 97.85\%, a recall rate of 97.85\%, and an F1-score of 97.90\%. The results presented in this study showcase the exceptional accuracy of the proposed methodology in accurately classifying the most commonly occurring brain tumor types. The technique exhibited commendable generalization properties, rendering it a valuable asset in medicine for aiding physicians in making precise and proficient brain diagnoses.},
DOI = {10.3390/brainsci13091320}
}


@ARTICLE{10.3389/fnhum.2023.1150120,
  
AUTHOR={Krishnapriya, Srigiri and Karuna, Yepuganti},   
	 
TITLE={Pre-trained deep learning models for brain MRI image classification},      
	
JOURNAL={Frontiers in Human Neuroscience},      
	
VOLUME={17},           
	
YEAR={2023},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnhum.2023.1150120},       
	
DOI={10.3389/fnhum.2023.1150120},      
	
ISSN={1662-5161},   
   
ABSTRACT={Brain tumors are serious conditions caused by uncontrolled and abnormal cell division. Tumors can have devastating implications if not accurately and promptly detected. Magnetic resonance imaging (MRI) is one of the methods frequently used to detect brain tumors owing to its excellent resolution. In the past few decades, substantial research has been conducted in the field of classifying brain images, ranging from traditional methods to deep-learning techniques such as convolutional neural networks (CNN). To accomplish classification, machine-learning methods require manually created features. In contrast, CNN achieves classification by extracting visual features from unprocessed images. The size of the training dataset had a significant impact on the features that CNN extracts. The CNN tends to overfit when its size is small. Deep CNNs (DCNN) with transfer learning have therefore been developed. The aim of this work was to investigate the brain MR image categorization potential of pre-trained DCNN VGG-19, VGG-16, ResNet50, and Inception V3 models using data augmentation and transfer learning techniques. Validation of the test set utilizing accuracy, recall, Precision, and F1 score showed that the pre-trained VGG-19 model with transfer learning exhibited the best performance. In addition, these methods offer an end-to-end classification of raw images without the need for manual attribute extraction.}
}

@ARTICLE{9926057,
  author={Kujur, Anima and Raza, Zahid and Khan, Arfat Ahmad and Wechtaisong, Chitapong},
  journal={IEEE Access}, 
  title={Data Complexity Based Evaluation of the Model Dependence of Brain MRI Images for Classification of Brain Tumor and Alzheimer’s Disease}, 
  year={2022},
  volume={10},
  number={},
  pages={112117-112133},
  keywords={Tumors;Diseases;Magnetic resonance imaging;Alzheimer's disease;Brain modeling;Complexity theory;Data models;Convolutional neural networks;Principle component analysis;Magnetic imaging resonance (MRI);brain tumor;alzheimer’s disease;S-CNN;ResNet50;inceptionV3;Xception;stratified k-fold;principal component analysis (PCA)},
  doi={10.1109/ACCESS.2022.3216393}}
  
  @article{Vimala_Srinivasan_Mathivanan_Mahalakshmi_Jayagopal_Dalu_2023, title={Detection and classification of brain tumor using hybrid deep learning models}, volume={13}, url={https://www.nature.com/articles/s41598-023-50505-6#citeas}, DOI={10.1038/s41598-023-50505-6}, number={1}, journal={Scientific Reports}, author={Vimala, Baiju Babu and Srinivasan, Saravanan and Mathivanan, Sandeep Kumar and Mahalakshmi, None and Jayagopal, Prabhu and Dalu, Gemmachis Teshite}, year={2023}, month=dec }


@Article{healthcare10091801,
AUTHOR = {Jibon, Ferdaus Anam and Khandaker, Mayeen Uddin and Miraz, Mahadi Hasan and Thakur, Himon and Rabby, Fazle and Tamam, Nissren and Sulieman, Abdelmoneim and Itas, Yahaya Saadu and Osman, Hamid},
TITLE = {Cancerous and Non-Cancerous Brain MRI Classification Method Based on Convolutional Neural Network and Log-Polar Transformation},
JOURNAL = {Healthcare},
VOLUME = {10},
YEAR = {2022},
NUMBER = {9},
ARTICLE-NUMBER = {1801},
URL = {https://www.mdpi.com/2227-9032/10/9/1801},
PubMedID = {36141413},
ISSN = {2227-9032},
ABSTRACT = {Magnetic resonance imaging (MRI) offers visual representations of the interior of a body for clinical analysis and medical intervention. The MRI process is subjected to a variety of image processing and machine learning approaches to identify, diagnose, and classify brain diseases as well as detect abnormalities. In this paper, we propose an improved classification method for distinguishing cancerous and noncancerous tumors from brain MRI images by using Log Polar Transformation (LPT) and convolutional neural networks (CNN). The LPT has been applied for feature extraction of rotation and scaling of distorted images, while the integration of CNN introduces a machine learning approach for the tumor classification of distorted images. The dataset was formed with images of seven different brain diseases, and the training set was formed by applying CNN with the extracted features. The proposed method is then evaluated in comparison to state-of-the-art algorithms, showing a definite improvement of the former. The obtained results show that the machine learning approach offers better classification with a success rate of about 96\% in both plain brain MR images and rotation- and scale-invariant brain MR images. This work also successfully classified T-1 and T-2 weighted images of neoplastic and degenerative brain diseases. The obtained accuracy is perfected by several kernel procedures, while the combined performance of the two wavelet transformations and a strong dataset make our method robust and efficient. Since no earlier study on machine learning approaches with rotated and scaled brain MRI has come to our attention, it is expected that our proposed method introduces a new paradigm in this research field.},
DOI = {10.3390/healthcare10091801}
}

@article{Cheng_Huang_Cao_Yang_Yang_Yun_Wang_Feng_2015, title={Enhanced performance of brain tumor classification via tumor region augmentation and partition}, volume={10}, url={https://doi.org/10.1371/journal.pone.0140381}, DOI={10.1371/journal.pone.0140381}, number={10}, journal={PloS One}, author={Cheng, Jun and Huang, Wei and Cao, Shuangliang and Yang, Ru and Yang, Wei and Yun, Zhaoqiang and Wang, Zhijian and Feng, Qianjin}, year={2015}, month=oct, pages={e0140381} }

@article{Oladimeji_Ibitoye_2023, title={Brain tumor classification using ResNet50-convolutional block attention module}, url={https://doi.org/10.1108/aci-09-2023-0022}, DOI={10.1108/aci-09-2023-0022}, journal={Applied Computing and Informatics}, author={Oladimeji, Oladosu Oyebisi and Ibitoye, Ayodeji Olusegun J.}, year={2023}, month=dec }

  
@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2024-06-12},
	date = {2015-04-10},
	eprinttype = {arxiv},
	eprint = {1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{dosovitskiy2021image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@Article{curroncol29100590,
AUTHOR = {Tummala, Sudhakar and Kadry, Seifedine and Bukhari, Syed Ahmad Chan and Rauf, Hafiz Tayyab},
TITLE = {Classification of Brain Tumor from Magnetic Resonance Imaging Using Vision Transformers Ensembling},
JOURNAL = {Current Oncology},
VOLUME = {29},
YEAR = {2022},
NUMBER = {10},
PAGES = {7498--7511},
URL = {https://www.mdpi.com/1718-7729/29/10/590},
PubMedID = {36290867},
ISSN = {1718-7729},
DOI = {10.3390/curroncol29100590}
}

%@article{François Chollet,
%  author       = {Fran{\c{c}}ois Chollet},
%  title        = {Xception: Deep Learning with Depthwise Separable Convolutions},
%  journal      = {CoRR},
%  volume       = {abs/1610.02357},
%  year         = {2016},
%  url          = {http://arxiv.org/abs/1610.02357},
%  eprinttype    = {arXiv},
%  eprint       = {1610.02357},
%  timestamp    = {Mon, 13 Aug 2018 16:46:20 +0200},
%  biburl       = {https://dblp.org/rec/journals/corr/Chollet16a.bib},
%  bibsource    = {dblp computer science bibliography, https://dblp.org}
%}

@INPROCEEDINGS{ImageNet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  abstract={The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848},
  ISSN={1063-6919},
  month={June},
}

@article{Pre-trainedModel,
	author = {Kenan Morani and Esra Kaya Ayana and Devrim Unay},
	title = {Covid-19 detection using modified xception transfer  learning approach from computed tomography images},
	journal = {International Journal of Advances in Intelligent Informatics},
	volume = {9},
	number = {3},
	year = {2023},
	keywords = {COVID-19 detection; Computed tomography images; Xception; Macro F1 score},
	abstract = {The significance of efficient and accurate diagnosis amidst the unique challenges posed by the COVID-19 pandemic underscores the urgency for innovative approaches. In response to these challenges, we propose a transfer learning-based approach using a recently annotated Computed Tomography (CT) image database. While many approaches propose an intensive data preprocessing and/or complex model architecture, our method focuses on offering an efficient solution with minimal manual engineering. Specifically, we investigate the suitability of a modified Xception model for COVID-19 detection. The method involves adapting a pre-trained Xception model, incorporating both the architecture and pre-trained weights from ImageNet. The output of the model was designed to make the final diagnosis decisions. The training utilized 128 batch sizes and 224x224 input image dimensions, downsized from standard 512x512. No further da processing was performed on the input data. Evaluation is conducted on the 'COV19-CT-DB' CT image dataset, containing labeled COVID-19 and non-COVID-19 cases. Results reveal the method's superiority in accuracy, precision, recall, and macro F1 score on the validation subset, outperforming the VGG-16 transfer model and thus offering enhanced precision with fewer parameters. Furthermore, compared to alternative methods for the COV19-CT-DB dataset, our approach exceeds the baseline approach and other alternatives on the same dataset. Finally, the adaptability of the modified Xception transfer learning-based model to the unique features of the COV19-CT-DB dataset showcases its potential as a robust tool for enhanced COVID-19 diagnosis from CT images.},
	issn = {2548-3161},	pages = {524--536},	doi = {10.26555/ijain.v9i3.1432},
	url = {https://ijain.org/index.php/IJAIN/article/view/1432}
}

@INPROCEEDINGS{9259870,
  author={Bulut, Batuhan and Kalın, Volkan and Güneş, Burcu Bektaş and Khazhin, Rim},
  booktitle={2020 Innovations in Intelligent Systems and Applications Conference (ASYU)}, 
  title={Deep Learning Approach For Detection Of Retinal Abnormalities Based On Color Fundus Images}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={In cases where people cannot access regular controls, treatment and care, delaying the diagnosis and treatment of eye diseases such as glaucoma, cataracts, diabetic retinopathy or leaving them to deteriorate unconsciously, may make daily life difficult and even cause blindness. Therefore, automatic examination of fundus photographs is important in terms of providing early diagnosis with fast, objective and consistent image evaluation and helping the application of large-scale scanning programs. This research uses Xception model with transfer learning method to classify images obtained from Akdeniz University Hospital Eye Diseases Department. During the analysis, the Xception model containing 50 different parameter combinations was trained by scanning the appropriate hyper-parameter space for the model. Comparisons were made for the top 9 models with the highest performance. The 4th model reached the highest accuracy rate with 91.39\% for the training set, and as for the validation set, the 0th model showed 82.5\% of accuracy. In addition, in order to test the performance of the model with an independent data set, open access fundus images were used for test analysis and binary classification AUC (Area Under Curve) values were calculated for 21 different diseases.},
  keywords={Training;Diabetes;Retinopathy;Data models;Mathematical model;Classification algorithms;Retina;deep learning;eye disease;fundus images;artificial intelligence;hyper-parameter optimization;transfer learning;Xception;decision support system;convolutional neural network;image classification;computer vision},
  doi={10.1109/ASYU50717.2020.9259870},
  ISSN={},
  month={Oct},}


@article{Wang2021Not,title={Not All Images are Worth $16x16 Words$: Dynamic Vision Transformers with Adaptive Sequence Length},author={Yulin Wang and Rui Huang and Shiji Song and Zeyi Huang and Gao Huang},journal={ArXiv},year={2021},volume={abs/2105.15075},doi={}}

@article{Al-Hadhrami2023An,title={An Effective Med-VQA Method Using a Transformer with Weights Fusion of Multiple Fine-Tuned Models},author={Suheer Al-Hadhrami and M. Menai and Saad Al-ahmadi and Ahmad Alnafessah},journal={Applied Sciences},year={2023},doi={10.3390/app13179735}}
