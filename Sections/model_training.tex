\section{Data Mining and Evaluation}\label{data_mining}

In this section, various data mining techniques are employed to extract meaningful insights from the preprocessed data.

\subsection{Overview}\label{overview}

All models were trained on Google's Colab plaform, utilizing the GPU runtime for faster training. In the following subsections, we will discuss the data mining techniques used to model the brain tumor classification task. The techniques include data augmentation, data splitting, and model selection.

\subsection{Model Selection}\label{model_selection}

Model selection is a critical step in developing an effective machine learning model, particularly for complex tasks such as brain tumor classification. The choice of model directly impacts the accuracy, efficiency, and overall performance of the solution. This section outlines our process of leveraging transfer learning with pretrained models, specifically VGG16 and ResNet50, alongside deep learning approaches like U-Net, which have demonstrated high performance in related tasks, including the BraTS (Brain Tumor Segmentation) Competition.

The BraTS Competition has established benchmarks for brain tumor segmentation, highlighting models and techniques that consistently achieve superior results. Models such as U-Net, VGG16, and EfficientNet are frequently employed by top teams due to their effectiveness in medical imaging tasks, making them ideal candidates for our project.

\paragraph{Transfer Learning with Pretrained Models:}
Transfer learning involves utilizing models pretrained on large datasets and fine-tuning them for specific tasks. This approach is particularly advantageous for small datasets, as it enables the model to leverage the knowledge acquired during pretraining. Our selection included InceptionV3, ResNet50, U-Net with an EfficientNet backbone, Xception, and Vision Transformer (ViT), chosen for their proven performance and efficiency.

InceptionV3 and ResNet50 were selected due to their established efficacy in various image classification tasks, with ResNet50's residual learning framework offering efficient deep network training. U-Net, combined with an EfficientNet backbone, was chosen for its superior performance in medical image segmentation, effectively capturing and localizing features in MRI scans. Xception, known for its depthwise separable convolutions, provides enhanced performance with reduced computational complexity. ViT, representing a cutting-edge approach using transformer architecture for image classification, offers robust handling of intricate data patterns.

These models were meticulously tested and fine-tuned to adapt to the specific requirements of brain tumor classification, ensuring that each model leveraged its architecture to maximize performance on our dataset.

\paragraph{Model Selection Process:}
Our model selection process involved an extensive literature review and practical testing of various models based on the dataset characteristics and specific requirements of the classification task. Given the relatively small size of dataset\_19, it was imperative to choose models capable of generalizing effectively from limited data.

We reviewed seminal and recent works on advanced deep learning architectures, examining the recommended image size, batch size, and hyperparameters to inform our model configuration. Notable studies, such as Khaliki et al. \cite{khaliki_brain_2024}, demonstrated the efficacy of the InceptionV3 architecture with specific modifications for brain tumor segmentation. We also explored GitHub repositories for practical insights and high-performing implementations.

Extensive testing compared different models and configurations, including InceptionV3, ResNet50, and EfficientNet, using parameters identified in the literature. Hyperparameters such as learning rate, dropout rate, and regularization techniques were fine-tuned using the Optuna package, enabling a systematic exploration of the hyperparameter space.

Each model's performance was evaluated using metrics detailed in the \hyperref[metrics]{Metrics} section, including precision, recall, F1 score, Dice Similarity Coefficient, specificity, and accuracy. The model with the highest validation accuracy and robust performance across these metrics was selected as the final model. This rigorous selection process ensured the chosen model performed well on training data and generalized effectively to unseen data, providing reliable and accurate classifications for brain tumor segmentation.

\subsection{Metrics}\label{metrics}

After training the models, their performance is evaluated using a variety of metrics, providing a comprehensive understanding of their effectiveness in classification tasks. Below are the commonly used metrics, along with their mathematical formulations based on True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN) values.

\subsubsection{Confusion Matrix}

The confusion matrix offers a detailed breakdown of the model's predictions, illustrating the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). This matrix is crucial for identifying the model's strengths and weaknesses in classifying different classes, allowing for a granular analysis of performance. By providing a visual representation of these components, the confusion matrix facilitates a deeper understanding of the model's prediction distribution and helps identify any potential biases or areas needing improvement.

\subsubsection{Precision}

Precision measures the proportion of true positive predictions among all positive predictions made by the model. It is calculated using the formula:

\[
\text{Precision} = \frac{TP}{TP + FP}
\]

A higher precision value is preferred as it indicates the model's ability to avoid false positives, making it a critical measure in scenarios where the cost of false positives is high. In the context of medical diagnoses, precision ensures that positive predictions are highly reliable, thereby reducing unnecessary treatments and associated costs.

\subsubsection{Recall}

Recall, also known as sensitivity, measures the proportion of true positive predictions among all actual positive instances in the dataset. It is computed using the formula:

\[
\text{Recall} = \frac{TP}{TP + FN}
\]

A higher recall value is desirable as it indicates the model's ability to capture all positive instances, which is particularly important in medical diagnoses where missing a positive case can have severe consequences. High recall ensures that the model identifies as many true cases as possible, minimizing the risk of overlooking critical conditions.

\subsubsection{F1 Score}

The F1 score, which is the harmonic mean of precision and recall, provides a balanced measure of the model's performance by considering both false positives and false negatives. The formula for the F1 score is:

\[
\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]

A higher F1 score signifies better performance, especially in cases where a balance between precision and recall is essential. This metric is particularly useful when dealing with imbalanced datasets, ensuring that the model maintains an equilibrium between precision and recall.

\subsubsection{Support}

Support refers to the number of instances in each class. It helps identify class imbalances and assess the model's performance across different classes, providing context to other metrics. Analyzing support alongside other metrics ensures that performance assessments are not skewed by class distribution.

\subsubsection{Dice Similarity Coefficient (DSC)}

The Dice Similarity Coefficient (DSC) is a metric commonly used in medical image segmentation tasks. It measures the overlap between the predicted and ground truth segmentation masks, with a higher DSC indicating better segmentation accuracy. The formula for DSC is:

\[
\text{DSC} = \frac{2 \times TP}{2 \times TP + FP + FN}
\]

DSC is critical in medical imaging as it quantifies how well the model segments the region of interest, providing a direct measure of segmentation quality.

\subsubsection{Specificity}

Specificity measures the proportion of true negative predictions among all actual negative instances in the dataset. It is calculated as:

\[
\text{Specificity} = \frac{TN}{TN + FP}
\]

A higher specificity value is preferred as it indicates the model's ability to avoid false positives, which is crucial in ensuring that negative cases are accurately identified. High specificity is essential in contexts where false positives can lead to unnecessary and potentially harmful interventions.

\subsubsection{Accuracy}

Accuracy measures the proportion of correct predictions made by the model across all classes, providing an overall assessment of the model's performance. It is computed using the formula:

\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]

A higher accuracy value signifies a better-performing model, offering a straightforward measure of its general effectiveness.

\subsubsection{Receiver Operating Characteristic (ROC) Curve and Area Under the ROC Curve (AUC)}

The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The Area Under the ROC Curve (AUC) measures the entire two-dimensional area underneath the ROC curve, providing an aggregate measure of performance across all classification thresholds. A higher AUC value is preferred as it indicates better model performance. The ROC curve and AUC are valuable for assessing the model's ability to distinguish between classes, especially in imbalanced datasets.

\subsubsection{Learning Curve}

The learning curve represents the model's performance in terms of training and validation loss and accuracies over epochs. By plotting these metrics, one can observe the model's learning behavior, identifying potential overfitting or underfitting. The learning curve is crucial for fine-tuning hyperparameters and ensuring that the model generalizes well to unseen data. A model that exhibits a decreasing training loss and stabilizing validation loss indicates effective learning, while significant gaps between training and validation performance may suggest overfitting.

Together, these metrics provide a comprehensive evaluation framework. Precision, recall, and F1 score primarily assess the model's performance on individual classes, while the confusion matrix and support provide detailed insights into prediction distribution. DSC is particularly crucial for segmentation tasks, ensuring accurate overlap measurement between predicted and actual segmentation masks. Specificity and accuracy offer additional layers of performance evaluation, ensuring robust and reliable model assessment. The ROC curve and AUC, along with the learning curve, provide further insights into the model's diagnostic capabilities and learning dynamics.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Individual Model SUBSECTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\import{Sections/models/}{unet.tex}

\import{Sections/models/}{inceptionv3.tex}

\import{Sections/models/}{resnet50.tex}

\import{Sections/models/}{vit.tex}

\import{Sections/models/}{others.tex}

\import{Sections/models/}{densenet121.tex}
